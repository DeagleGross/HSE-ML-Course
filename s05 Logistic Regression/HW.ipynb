{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis for Software Engineers\n",
    "\n",
    "## Practical Assignment 4\n",
    "## Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take into account that some tasks may not have a rigorous and comprehensive solution.\n",
    "\n",
    "Support your code with comments and illustrations if needed. The more conclusions, derivations and explanations you provide - the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Stochastic) Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the logistic regression method for binary classification. \n",
    "\n",
    "In this part, you need to implement the following algorithms for log-loss optimization\n",
    "* gradient descent (GD) \n",
    "* stochastic gradient descent (SGD)\n",
    "\n",
    "In these three methods a gradient of a loss function $L(w) = \\frac{1}{N}\\sum_i^N l_i(w)$ is obtained differently:\n",
    "\n",
    "* In GD the whole training dataset is used: $\\nabla_wL(w) = \\nabla_w\\frac{1}{N}\\sum_i l_i(w)$\n",
    "\n",
    "* In SGD only one random training object $i$ is used for gradient estimation: $\\nabla_wL(w) \\approx \\nabla_w l_i(w)$. So one **epoch** in SGD proceeds as follows:\n",
    "    * Shuffle training dataset\n",
    "    * Iterate over dataset objects one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Loss (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider logistic regression with $L_1$ and $L_2$ regularization - elastic net.\n",
    "\n",
    "$$\n",
    "L(w, w_0) = \\frac{1}{N} \\sum_i^N \\ln(1+\\exp(-y_i(w^\\top x_i+w_0))) + \\gamma \\|w\\|_1 + \\beta \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "Find its gradient and update rules for gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer with LaTex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Consider:\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\|w\\|_1)'_w = sign(w)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\|w\\|^2_2)'_w = 2w\n",
    "$$\n",
    "\n",
    "$$\n",
    "-y_i*(w^Tx_i) = -y_i x_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Result:\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw_j} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\exp(-y_i(w^Tx_i + w_0))(-y_ix_{ij})}{1 + \\exp(-y_i(w^Tx_i + w_0))}+ \\gamma sign(w_j) + 2 \\beta w_j\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\frac{dL}{dw_0} = \\frac{1}{N} \\sum_{i=1}^N \\frac{-y_i\\exp(-y_i(w^Tx_i + w_0))}{1 + \\exp(-y_i(w^Tx_i + w_0))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement corresponding python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(X, y, coef, intercept, gamma=1., beta=1.):\n",
    "    # grad_coef is a 1-dim array; grad_intercept is a float number\n",
    "    N = len(y)\n",
    "    C = len(coef)\n",
    "    grad_coef = np.zeros(C)\n",
    "    grad_intercept = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        grad_intercept += (-y[i] * np.exp(-y[i]*(np.dot(coef,X[i]) + intercept))) / (1 + np.exp(-y[i]*(np.dot(coef,X[i]) + intercept)))    \n",
    "    grad_intercept /= N\n",
    "    \n",
    "    for i in range(C):\n",
    "        for j in range(N):\n",
    "            exponent = np.exp(-y[j]*(coef @ X[j] + intercept))\n",
    "            grad_coef[i] += ((-y[j]*X[j][i]) * exponent) / (1 + exponent)\n",
    "        grad_coef[i] = grad_coef[i] / N + gamma*np.sign(coef[i]) + 2*beta*coef[i]\n",
    "    \n",
    "    return grad_coef, grad_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can test your grad computation (comare your exact computation with numerical approximation)\n",
    "\n",
    "def loss(X, y, coef, intercept, gamma=1., beta=1.):\n",
    "    return np.log(1+np.exp(-y*(X @ coef+ intercept))).mean() + gamma * np.linalg.norm(coef, ord=1) + beta * (np.linalg.norm(coef, ord=2) ** 2)\n",
    "\n",
    "def get_grad_numerical(X, y, coef, intercept, gamma=1., beta=1., eps=0.000001):\n",
    "    mean_grad_coef = []\n",
    "    for i, _ in enumerate(coef):\n",
    "        coef_ = coef.copy()\n",
    "        coef_[i]+=eps\n",
    "        mean_grad_coef.append((loss(X, y, coef_, intercept, gamma=1., beta=1.) - loss(X, y, coef, intercept, gamma=1., beta=1.))/eps)\n",
    "        \n",
    "    intercept_ = intercept+eps\n",
    "    mean_grad_intercept = (loss(X, y, coef, intercept_, gamma=1., beta=1.) - loss(X, y, coef, intercept, gamma=1., beta=1.))/eps\n",
    "    \n",
    "    return np.array(mean_grad_coef), mean_grad_intercept\n",
    "\n",
    "np.random.seed(1928)\n",
    "X = np.random.multivariate_normal(np.arange(5), np.eye(5), size=10)\n",
    "y = np.random.binomial(1, 0.42, size=10)\n",
    "coef, intercept = np.random.normal(size=5), np.random.normal()\n",
    "\n",
    "grad_coef, grad_intercept = get_grad(X, y, coef, intercept)\n",
    "grad_coef_numerical, grad_intercept_numerical = get_grad_numerical(X, y, coef, intercept)\n",
    "\n",
    "assert(np.allclose(grad_coef,\n",
    "                   grad_coef_numerical,\n",
    "                   rtol=1e-2) & \\\n",
    "       np.allclose(grad_intercept,\n",
    "                   grad_intercept_numerical, \n",
    "                   rtol=1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implementation (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints:\n",
    "* Small random numbers from $[−1/2d,1/2d]$ should be chosen for weight initialization. ($d$ - feature space dimension)\n",
    "* The efficient step size for GD is approximately $0.01 − 1$.\n",
    "* Step size should be constant for GD and decreasing for SGD, for example, $\\alpha/\\text{epoch_number}$ where $\\alpha$ is some constant\n",
    "*  use [`sklearn.utils.shuffle`](http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html) to shuffle `X` and `y` in a consistent way\n",
    "* Stopping criteria: for GD use $|L_{old} − L_{new}| < tol$, for SGD simply do a particular number of iterations.\n",
    "* For code efficiency use numpy vectors to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_vector(d):\n",
    "    return (np.random.random(d) - 0.5) * d\n",
    "\n",
    "def random_number():\n",
    "    return np.random.random() - 0.5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for simple gradient descent\n",
    "class MyVanillaGD(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, step=0.05, beta=1.0, gamma=1.0, tol=0.01, max_epoch=1000, random_state=123):\n",
    "        self.beta = beta        \n",
    "        self.gamma = gamma\n",
    "        self.tol = tol\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # my fields\n",
    "        np.random.seed(random_state)\n",
    "        self.step = step\n",
    "        \n",
    "        self.coef = []\n",
    "        self.intercept = 0\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # randoming coefs\n",
    "        self.coef = random_vector(X.shape[1])\n",
    "        self.intercept = random_number()\n",
    "        \n",
    "        # setting loss variables\n",
    "        L_prev = loss(X, y, self.coef, self.intercept, self.gamma, self.beta)\n",
    "        L_cur = 0\n",
    "        loss_process = [L_prev, ]\n",
    "        \n",
    "        # using max_epoch as another stopping criteria\n",
    "        for i in range(self.max_epoch):           \n",
    "            # update for L_cur, coef and intercept\n",
    "            grad_coef, grad_intercept = get_grad(X, y, self.coef, self.intercept, self.gamma, self.beta)\n",
    "            \n",
    "            # moving on constant step (as written on top - for GD constant step)\n",
    "            self.coef -= np.sign(grad_coef) * self.step\n",
    "            self.intercept -= np.sign(grad_intercept) * self.step\n",
    "            \n",
    "            # and counting new loss\n",
    "            L_cur = loss(X, y, self.coef, self.intercept, self.gamma, self.beta)\n",
    "            loss_process.append(L_cur)\n",
    "            \n",
    "            # breaking if tolerance rule not passed\n",
    "            if (np.abs(L_prev - L_cur) < self.tol):\n",
    "                break\n",
    "                \n",
    "            # reassigning L_prev and L_cur\n",
    "            L_prev = L_cur\n",
    "        \n",
    "        return loss_process\n",
    "    \n",
    "    def predict(self, X):\n",
    "        v = (self.predict_proba(X) > 0.5).astype(int)\n",
    "        v[v == 0] = -1\n",
    "        \n",
    "        return v\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return 1 / (1 + np.exp(- X @ self.coef - self.intercept))\n",
    "\n",
    "        \n",
    "# Class for SGD\n",
    "class MySGD(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, step=0.01, beta=1.0, gamma=1.0, max_epoch=1000, random_state=123):\n",
    "        self.beta = beta        \n",
    "        self.gamma = gamma\n",
    "        self.max_epoch = max_epoch\n",
    "        self.random_state = random_state\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.step = step\n",
    "        \n",
    "        self.coef = []\n",
    "        self.intercept = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # randoming coefs\n",
    "        self.coef = random_vector(X.shape[1])\n",
    "        self.intercept = random_number()\n",
    "        \n",
    "        # setting loss array\n",
    "        loss_process = [loss(X, y, self.coef, self.intercept, self.gamma, self.beta), ]\n",
    "        \n",
    "        # using max_epoch as another stopping criteria\n",
    "        for i in range(self.max_epoch):\n",
    "            for j in range(y.shape[0]):\n",
    "                # update for gradients\n",
    "                grad_coef, grad_intercept = get_grad(np.array([X[j],]), np.array([y[j],]), self.coef, self.intercept, self.gamma, self.beta)\n",
    "\n",
    "                # moving on decreasing step\n",
    "                self.coef -= grad_coef * self.step\n",
    "                self.intercept -= grad_intercept * self.step\n",
    "                \n",
    "            loss_process.append(loss(X, y, self.coef, self.intercept, self.gamma, self.beta))\n",
    "        \n",
    "        return loss_process\n",
    "    \n",
    "    def predict(self, X):\n",
    "        v = (self.predict_proba(X) > 0.5).astype(int)\n",
    "        v[v == 0] = -1\n",
    "        \n",
    "        return v\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return 1 / (1 + np.exp(- X @ self.coef - self.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking on simple datasets (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check your models on a toy dataset. Don’t forget to standardize the data and then add a constant feature to it. Use the same random state for GD and SGD\n",
    "\n",
    "Here you should demonstrate the following plots:\n",
    "* data points and decision boundary for each method,\n",
    "* $L(w)$ as a function of epoch number (for both GD and SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "RND_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_dataset():\n",
    "    return make_classification(n_samples=1000, n_features=2, class_sep=0.5, \n",
    "                               n_clusters_per_class=1, n_redundant=0, \n",
    "                               shift=4, scale=2, random_state=RND_SEED)\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    X1min, X2min = X.min(axis=0)\n",
    "    X1max, X2max = X.max(axis=0)\n",
    "    x1, x2 = np.meshgrid(np.linspace(X1min, X1max, 500),\n",
    "                         np.linspace(X2min, X2max, 500))\n",
    "    ypred = model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    ypred = ypred.reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, ypred, alpha=.4)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# using stardartization from sklearn\n",
    "X, y = get_toy_dataset()\n",
    "y[y == 0] = -1\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "gd = MyVanillaGD(step = 0.1, beta = 0., gamma = 0., tol = 1e-8, max_epoch = 2000, random_state = RND_SEED)\n",
    "lossGD = gd.fit(X, y)\n",
    "predict = gd.predict(X)\n",
    "print(accuracy_score(y, predict))\n",
    "plot_decision_boundary(gd, X, y)\n",
    "\n",
    "sgd = MySGD(step = 1e-3, beta = 0., gamma = 0., max_epoch = 2000, random_state = RND_SEED)\n",
    "lossSGD = sgd.fit(X, y)\n",
    "predict = sgd.predict(X)\n",
    "print(accuracy_score(y, predict))\n",
    "plot_decision_boundary(sgd, X, y)\n",
    "\n",
    "# drawing loss functions\n",
    "fig = plt.figure()\n",
    "plt.plot(lossGD, label='model GD')\n",
    "plt.plot(lossSGD, label='model SGD')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gd.coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulatization (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the dataset from 'audit' folder. Fit a logistic regression classifier on the training samples. Use GD with different regularizations (without one, only L1, only L2, L1 and L2), use the same random state for all runs. Don’t forget to standardize the data (for example use StandardScaler from sklearn.preprocessing) and then add a constant feature to it.\n",
    "* split data on train and test, using train_test_split from sklearn with test_size = 0.3 (don't forget set random_state)\n",
    "* Plot loss on test data_set wrt to epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - your logreg, X_test, y_test - your test data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_score = model.predict_proba(X_test)[:,1]\n",
    "assert(roc_auc_score(y_test, y_score) > 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the resulting weights vector of GD with L1 regularization to determine two most important features. Fit the logistic classifier only on these two features (+ the constant one) and visualize the decision boundary. Does L1 regularization help you to chose important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Provide visual demonstration, that your regularization works. For each feature plot its weight wrt to regularization coefficients\n",
    "\n",
    " * $\\beta = 0$, $\\gamma \\in [10^{-4}, 10^4]$\n",
    " * $\\beta \\in [10^{-4}, 10^4]$, $\\gamma = 0$\n",
    " * $\\beta \\in [10^{-4}, 10^4]$, $\\gamma = 0.2 \\cdot \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plots should look similar to\n",
    "<img src=\"img/example.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Dataset (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the task you will work with the problem of diabetes diagnostics. Load the diabetes dataset using pickle.load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has the following features:\n",
    "1. Number of pregnancies\n",
    "2. Plasma glucose concentration after 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure\n",
    "4. Triceps skin fold thickness\n",
    "5. 2-Hour serum insulin\n",
    "6. Body mass index\n",
    "7. Diabetes pedigree function\n",
    "8. Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class label is equal to 1 if a person has a diabetes and to -1 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Train the logistic regression classifier on this dataset. Use SGD without regularization. Don’t\n",
    "forget to standardize the data and then add a constant feature to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diabetes.pkl', 'rb') as fin:     \n",
    "    d = pickle.load(fin,encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - your logreg, X_test, y_test - your test data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_score = model.predict_proba(X_test)[:,1]\n",
    "assert(roc_auc_score(y_test, y_score) > 0.793)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diagnostic problems, false positive and false negative errors have different costs. \n",
    "\n",
    "Let’s say, if we make a false negative error (don’t detect a condition when it is present), then the patient doesn’t have a necessary treatment and, if we make false positive error (detect a condition when it isn’t present), then the patient simply need to be tested more. \n",
    "\n",
    "Therefore, the cost of false negative error is higher, and we care much more about this type of error. Compute a confusion matrix for a fitted classifier. \n",
    "\n",
    "How many errors of each type have you got? Compute a false positive and false negative rates for this classifier. Why are they so different?\n",
    "\n",
    "Useful functions: `sklearn.metrics.confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the proportion of errors of different types you can change a threshold a at the prediction\n",
    "rule $y = \\sigma(w^\\top x + w_0) > a$, where $a \\in [0, 1]$.\n",
    "\n",
    "Show the ROC-curve of the fitted classifier and a point on it, which corresponds to $a = 0.5$ (the one you\n",
    "computed at the previous step). \n",
    "\n",
    "Using ROC-curve choose a so that false negative rate is less than $20%$ while a false positive rate is still small. What accuracy and false positive rate does the final algorithm have?\n",
    "\n",
    "Useful functions: `sklearn.metrics.roc_curve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "235px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
